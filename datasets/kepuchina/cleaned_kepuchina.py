import requests
import pandas as pd
from bs4 import BeautifulSoup
import time
from datetime import datetime
import re

def clean_text(text):
    """æ·±åº¦æ¸…ç†æ–‡æœ¬å†…å®¹"""
    if not text or not isinstance(text, str):
        return ""
    
    # å»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    # æ›¿æ¢å¤šç§ç©ºç™½å­—ç¬¦ä¸ºå•ä¸ªç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œå¸¸è§æ ‡ç‚¹
    text = re.sub(r'[^\w\u4e00-\u9fff\s\-â€”ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š"\'ï¼ˆï¼‰ã€Šã€‹]', '', text)
    return text.strip()

def format_date(date_str):
    """ä¸¥æ ¼è§„èŒƒåŒ–æ—¥æœŸæ ¼å¼"""
    if not date_str or not isinstance(date_str, str):
        return ""
    
    date_str = date_str.strip()
    patterns = [
        (r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', '%Y-%m-%d'),
        (r'(\d{4})-(\d{1,2})-(\d{1,2})', '%Y-%m-%d'),
        (r'(\d{4})/(\d{1,2})/(\d{1,2})', '%Y/%m/%d'),
        (r'(\d{1,2})æœˆ(\d{1,2})æ—¥', f'{datetime.now().year}-%m-%d'),
        (r'(\d{4})\.(\d{1,2})\.(\d{1,2})', '%Y.%m.%d')
    ]
    
    for pattern, fmt in patterns:
        match = re.search(pattern, date_str)
        if match:
            try:
                if fmt == '%Y-%m-%d':
                    year = match.group(1)
                    month = match.group(2).zfill(2)
                    day = match.group(3).zfill(2)
                    return f"{year}-{month}-{day}"
                elif fmt == f'{datetime.now().year}-%m-%d':
                    month = match.group(1).zfill(2)
                    day = match.group(2).zfill(2)
                    return f"{datetime.now().year}-{month}-{day}"
                else:
                    date_obj = datetime.strptime(date_str, fmt)
                    return date_obj.strftime('%Y-%m-%d')
            except ValueError:
                continue
    return ""

def validate_url(url):
    """éªŒè¯å¹¶æ ‡å‡†åŒ–URL"""
    if not url or not isinstance(url, str):
        return ""
    
    url = url.strip()
    if not url.startswith(('http://', 'https://')):
        if url.startswith('/'):
            return f"https://piyao.kepuchina.cn{url}"
        else:
            return f"https://piyao.kepuchina.cn/{url}"
    return url

def get_piyao_rumors():
    base_url = "https://piyao.kepuchina.cn/rumor/rumorlist"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
        "Referer": "https://piyao.kepuchina.cn/",
        "Accept-Language": "zh-CN,zh;q=0.9"
    }
    
    params = {"type": "0", "page": 1}
    all_rumors = []
    max_pages = 5
    
    for page in range(1, max_pages + 1):
        params["page"] = page
        print(f"æ­£åœ¨å¤„ç†ç¬¬ {page} é¡µ...")
        
        try:
            response = requests.get(base_url, headers=headers, params=params, timeout=15)
            response.encoding = "utf-8"
            
            if response.status_code != 200:
                print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                continue
                
            soup = BeautifulSoup(response.text, "html.parser")
            items = soup.select(".rumor-item, .list-item, div[class*=item], li[class*=item]")
            
            for item in items:
                try:
                    # æ ‡é¢˜æå–ä¸æ¸…æ´—
                    title_elem = item.select_one(".title, .rumor-title, h3, h4, a")
                    title = clean_text(title_elem.get_text()) if title_elem else ""
                    if not title or title.lower() in ["æ— æ ‡é¢˜", "æš‚æ— æ ‡é¢˜"]:
                        continue
                    
                    # é“¾æ¥å¤„ç†
                    link = validate_url(title_elem.get("href", "")) if title_elem else ""
                    
                    # æ—¥æœŸå¤„ç†
                    date_elem = item.select_one(".date, .time, .rumor-date, span[class*=date], .pub-time")
                    date_str = clean_text(date_elem.get_text()) if date_elem else ""
                    date = format_date(date_str)
                    
                    # åªä¿ç•™æœ‰æ•ˆæ•°æ®
                    if title and (link or date):
                        all_rumors.append({
                            "æ ‡é¢˜": title,
                            "ç½‘ç«™": link,
                            "æ—¶é—´": date
                        })
                    
                except Exception as e:
                    print(f"è§£ææ¡ç›®å‡ºé”™: {str(e)}")
                    continue
                    
            time.sleep(3)  # å¢åŠ å»¶è¿Ÿé¿å…è¢«å°
            
        except Exception as e:
            print(f"ç¬¬ {page} é¡µå¤„ç†å¤±è´¥: {str(e)}")
            continue
    
    return all_rumors

def enhanced_data_cleaning(df):
    """å¢å¼ºçš„æ•°æ®æ¸…æ´—æµç¨‹"""
    # å»é™¤å®Œå…¨é‡å¤çš„æ•°æ®
    df = df.drop_duplicates(subset=['æ ‡é¢˜', 'ç½‘ç«™', 'æ—¶é—´'], keep='first')
    
    # å»é™¤æ ‡é¢˜ç›¸ä¼¼åº¦é«˜çš„æ•°æ®ï¼ˆç®€å•ç‰ˆï¼‰
    df = df.drop_duplicates(subset=['æ ‡é¢˜'], keep='first')
    
    # æ ‡å‡†åŒ–æ—¶é—´æ ¼å¼
    df['æ—¶é—´'] = pd.to_datetime(df['æ—¶é—´'], errors='coerce').dt.strftime('%Y-%m-%d')
    
    # æ ‡è®°å¹¶å¤„ç†å¼‚å¸¸å€¼
    df['æ ‡é¢˜é•¿åº¦'] = df['æ ‡é¢˜'].str.len()
    df = df[(df['æ ‡é¢˜é•¿åº¦'] >= 5) & (df['æ ‡é¢˜é•¿åº¦'] <= 100)]  # å»é™¤è¿‡çŸ­æˆ–è¿‡é•¿çš„æ ‡é¢˜
    df = df.drop(columns=['æ ‡é¢˜é•¿åº¦'])
    
    # å¡«å……ç©ºå€¼
    df['ç½‘ç«™'] = df['ç½‘ç«™'].replace('', pd.NA)
    df['æ—¶é—´'] = df['æ—¶é—´'].replace('', pd.NA)
    
    return df

def save_to_csv(df, filename):
    """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
    # æŒ‰æ—¶é—´é™åºæ’åˆ—
    df = df.sort_values(by='æ—¶é—´', ascending=False, na_position='last')
    
    # é‡ç½®ç´¢å¼•
    df = df.reset_index(drop=True)
    
    # ä¿å­˜ä¸ºUTF-8 with BOMç¼–ç ï¼Œç¡®ä¿Excelå…¼å®¹
    df.to_csv(filename, index=False, encoding='utf_8_sig')
    return df

if __name__ == "__main__":
    print("å¼€å§‹çˆ¬å–å¹¶æ¸…æ´—æ•°æ®...")
    start_time = time.time()
    
    rumors_data = get_piyao_rumors()
    
    if rumors_data:
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(rumors_data)
        
        # æ‰§è¡Œå¢å¼ºçš„æ•°æ®æ¸…æ´—
        df = enhanced_data_cleaning(df)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"piyao_cleaned_data_{timestamp}.csv"
        
        # ä¿å­˜æ•°æ®
        df = save_to_csv(df, filename)
        
        # æ‰“å°ç»“æœæŠ¥å‘Š
        print(f"\nâœ… æ•°æ®æ¸…æ´—å®Œæˆï¼è€—æ—¶: {time.time()-start_time:.1f}ç§’")
        print(f"ğŸ“Š æ€»è®°å½•æ•°: {len(df)}")
        print(f"ğŸ“ å·²ä¿å­˜åˆ°: {filename}")
        
        print("\nğŸ” æ•°æ®è´¨é‡æŠ¥å‘Š:")
        print(f"- æœ‰æ•ˆæ ‡é¢˜: {len(df[df['æ ‡é¢˜'].notna()])}")
        print(f"- æœ‰æ•ˆé“¾æ¥: {len(df[df['ç½‘ç«™'].notna()])}")
        print(f"- æœ‰æ•ˆæ—¶é—´: {len(df[df['æ—¶é—´'].notna()])}")
        
        print("\nğŸ“‹ æ•°æ®é¢„è§ˆ:")
        print(df.head(10).to_string(index=False))
        
        # ä¿å­˜è´¨é‡æŠ¥å‘Š
        report = {
            'æ€»è®°å½•æ•°': len(df),
            'æœ‰æ•ˆæ ‡é¢˜': len(df[df['æ ‡é¢˜'].notna()]),
            'æœ‰æ•ˆé“¾æ¥': len(df[df['ç½‘ç«™'].notna()]),
            'æœ‰æ•ˆæ—¶é—´': len(df[df['æ—¶é—´'].notna()]),
            'æœ€æ—©æ—¥æœŸ': df['æ—¶é—´'].min(),
            'æœ€è¿‘æ—¥æœŸ': df['æ—¶é—´'].max()
        }
        pd.DataFrame.from_dict(report, orient='index', columns=['ç»Ÿè®¡å€¼']).to_csv(
            f"data_quality_report_{timestamp}.csv", 
            encoding='utf_8_sig'
        )
    else:
        print("âŒ æœªèƒ½è·å–æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ç½‘ç«™ç»“æ„")